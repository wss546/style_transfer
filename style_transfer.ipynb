{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" An implementation of the paper \"A Neural Algorithm of Artistic Style\"\n",
    "by Gatys et al. in TensorFlow.\n",
    "\n",
    "Author: Chip Huyen (huyenn@stanford.edu)\n",
    "Prepared for the class CS 20SI: \"TensorFlow for Deep Learning Research\"\n",
    "For more details, please read the assignment handout:\n",
    "http://web.stanford.edu/class/cs20si/assignments/a2.pdf\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import vgg_model\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters to manage experiments\n",
    "STYLE = 'starry_night'\n",
    "CONTENT = 'deadpool'\n",
    "STYLE_IMAGE = 'styles/' + STYLE + '.jpg'\n",
    "CONTENT_IMAGE = 'content/' + CONTENT + '.jpg'\n",
    "IMAGE_HEIGHT = 250\n",
    "IMAGE_WIDTH = 333\n",
    "NOISE_RATIO = 0.6 # percentage of weight of the noise for intermixing with the content image\n",
    "\n",
    "CONTENT_WEIGHT = 0.01\n",
    "STYLE_WEIGHT = 1\n",
    "\n",
    "# Layers used for style features. You can change this.\n",
    "STYLE_LAYERS = ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1']\n",
    "W = [0.5, 1.0, 1.5, 3.0, 4.0] # give more weights to deeper layers.\n",
    "\n",
    "# Layer used for content features. You can change this.\n",
    "CONTENT_LAYER = 'conv4_2'\n",
    "\n",
    "ITERS = 300\n",
    "LR = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MEAN_PIXELS = np.array([123.68, 116.779, 103.939]).reshape((1,1,1,3))\n",
    "\"\"\" MEAN_PIXELS is defined according to description on their github:\n",
    "https://gist.github.com/ksimonyan/211839e770f7b538e2d8\n",
    "'In the paper, the model is denoted as the configuration D trained with scale jittering. \n",
    "The input images should be zero-centered by mean pixel (rather than mean image) subtraction. \n",
    "Namely, the following BGR values should be subtracted: [103.939, 116.779, 123.68].'\n",
    "\"\"\"\n",
    "\n",
    "# VGG-19 parameters file\n",
    "VGG_DOWNLOAD_LINK = 'http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-19.mat'\n",
    "VGG_MODEL = 'imagenet-vgg-verydeep-19.mat'\n",
    "EXPECTED_BYTES = 534904783"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _create_content_loss(p, f):\n",
    "    \"\"\" Calculate the loss between the feature representation of the\n",
    "    content image and the generated image.\n",
    "    \n",
    "    Inputs: \n",
    "        p, f are just P, F in the paper \n",
    "        (read the assignment handout if you're confused)\n",
    "        Note: we won't use the coefficient 0.5 as defined in the paper\n",
    "        but the coefficient as defined in the assignment handout.\n",
    "    Output:\n",
    "        the content loss\n",
    "\n",
    "    \"\"\"\n",
    "    return tf.reduce_sum((f - p) ** 2) / (4.0 * p.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _gram_matrix(F, N, M):\n",
    "    \"\"\" Create and return the gram matrix for tensor F\n",
    "        Hint: you'll first have to reshape F\n",
    "    \"\"\"\n",
    "    F = tf.reshape(F, (M, N))\n",
    "    return tf.matmul(tf.transpose(F), F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _single_style_loss(a, g):\n",
    "    \"\"\" Calculate the style loss at a certain layer\n",
    "    Inputs:\n",
    "        a is the feature representation of the real image\n",
    "        g is the feature representation of the generated image\n",
    "    Output:\n",
    "        the style loss at a certain layer (which is E_l in the paper)\n",
    "\n",
    "    Hint: 1. you'll have to use the function _gram_matrix()\n",
    "        2. we'll use the same coefficient for style loss as in the paper\n",
    "        3. a and g are feature representation, not gram matrices\n",
    "    \"\"\"\n",
    "    N = a.shape[3] # number of filters\n",
    "    M = a.shape[1] * a.shape[2] # height times width of the feature map\n",
    "    A = _gram_matrix(a, N, M)\n",
    "    G = _gram_matrix(g, N, M)\n",
    "    return tf.reduce_sum((G - A) ** 2 / ((2 * N * M) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _create_style_loss(A, model):\n",
    "    \"\"\" Return the total style loss\n",
    "    \"\"\"\n",
    "    n_layers = len(STYLE_LAYERS)\n",
    "    E = [_single_style_loss(A[i], model[STYLE_LAYERS[i]]) for i in range(n_layers)]\n",
    "    \n",
    "    ###############################\n",
    "    ## TO DO: return total style loss\n",
    "    return sum([W[i] * E[i] for i in range(n_layers)])\n",
    "    ###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _create_losses(model, input_image, content_image, style_image):\n",
    "    with tf.variable_scope('loss') as scope:\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(input_image.assign(content_image)) # assign content image to the input variable\n",
    "            p = sess.run(model[CONTENT_LAYER])\n",
    "        content_loss = _create_content_loss(p, model[CONTENT_LAYER])\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(input_image.assign(style_image))\n",
    "            A = sess.run([model[layer_name] for layer_name in STYLE_LAYERS])                              \n",
    "        style_loss = _create_style_loss(A, model)\n",
    "\n",
    "        ##########################################\n",
    "        ## TO DO: create total loss. \n",
    "        ## Hint: don't forget the content loss and style loss weights\n",
    "        total_loss = CONTENT_WEIGHT * content_loss + STYLE_WEIGHT * style_loss\n",
    "        ##########################################\n",
    "\n",
    "    return content_loss, style_loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _create_summary(model):\n",
    "    \"\"\" Create summary ops necessary\n",
    "        Hint: don't forget to merge them\n",
    "    \"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        tf.summary.scalar('content loss', model['content_loss'])\n",
    "        tf.summary.scalar('style loss', model['style_loss'])\n",
    "        tf.summary.scalar('total loss', model['total_loss'])\n",
    "        tf.summary.histogram('histogram content loss', model['content_loss'])\n",
    "        tf.summary.histogram('histogram style loss', model['style_loss'])\n",
    "        tf.summary.histogram('histogram total loss', model['total_loss'])\n",
    "        return tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, generated_image, initial_image):\n",
    "    \"\"\" Train your model.\n",
    "    Don't forget to create folders for checkpoints and outputs.\n",
    "    \"\"\"\n",
    "    skip_step = 1\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        ###############################\n",
    "        ## TO DO: \n",
    "        ## 1. initialize your variables\n",
    "        ## 2. create writer to write your graph\n",
    "        saver = tf.train.Saver()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        writer = tf.summary.FileWriter('graphs', sess.graph)\n",
    "        ###############################\n",
    "        sess.run(generated_image.assign(initial_image))\n",
    "        ## ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "        ##if ckpt and ckpt.model_checkpoint_path:\n",
    "        ##    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        initial_step = model['global_step'].eval()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for index in range(initial_step, ITERS):\n",
    "            if index >= 5 and index < 20:\n",
    "                skip_step = 10\n",
    "            elif index >= 20:\n",
    "                skip_step = 20\n",
    "            \n",
    "            sess.run(model['optimizer'])\n",
    "            if (index + 1) % skip_step == 0:\n",
    "                ###############################\n",
    "                ## TO DO: obtain generated image and loss\n",
    "                gen_image, total_loss, summary = sess.run([generated_image, model['total_loss'], \n",
    "                                                             model['summary_op']])\n",
    "\n",
    "                ###############################\n",
    "                gen_image = gen_image + MEAN_PIXELS\n",
    "                writer.add_summary(summary, global_step=index)\n",
    "                print('Step {}\\n   Sum: {:5.1f}'.format(index + 1, np.sum(gen_image)))\n",
    "                print('   Loss: {:5.1f}'.format(total_loss))\n",
    "                print('   Time: {}'.format(time.time() - start_time))\n",
    "                start_time = time.time()\n",
    "\n",
    "                filename = 'outputs/%d.png' % (index)\n",
    "                utils.save_image(filename, gen_image)\n",
    "\n",
    "                if (index + 1) % 20 == 0:\n",
    "                    saver.save(sess, 'checkpoints/style_transfer', index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG-19 pre-trained model ready\n",
      "INFO:tensorflow:Summary name content loss is illegal; using content_loss instead.\n",
      "INFO:tensorflow:Summary name style loss is illegal; using style_loss instead.\n",
      "INFO:tensorflow:Summary name total loss is illegal; using total_loss instead.\n",
      "INFO:tensorflow:Summary name histogram content loss is illegal; using histogram_content_loss instead.\n",
      "INFO:tensorflow:Summary name histogram style loss is illegal; using histogram_style_loss instead.\n",
      "INFO:tensorflow:Summary name histogram total loss is illegal; using histogram_total_loss instead.\n",
      "Step 1\n",
      "   Sum: 24642725.8\n",
      "   Loss: 1288235776.0\n",
      "   Time: 1.2389087677001953\n",
      "Step 2\n",
      "   Sum: 24630427.2\n",
      "   Loss: 984627200.0\n",
      "   Time: 0.13133668899536133\n",
      "Step 3\n",
      "   Sum: 24618733.7\n",
      "   Loss: 775614528.0\n",
      "   Time: 0.13500738143920898\n",
      "Step 4\n",
      "   Sum: 24608997.9\n",
      "   Loss: 633553728.0\n",
      "   Time: 0.12703442573547363\n",
      "Step 5\n",
      "   Sum: 24601229.2\n",
      "   Loss: 532323008.0\n",
      "   Time: 0.11797595024108887\n",
      "Step 10\n",
      "   Sum: 24576150.5\n",
      "   Loss: 306919040.0\n",
      "   Time: 0.3160669803619385\n",
      "Step 20\n",
      "   Sum: 24573892.1\n",
      "   Loss: 140438368.0\n",
      "   Time: 0.5561270713806152\n",
      "Step 40\n",
      "   Sum: 24558674.8\n",
      "   Loss: 53553416.0\n",
      "   Time: 1.8174386024475098\n",
      "Step 60\n",
      "   Sum: 24531837.2\n",
      "   Loss: 33081028.0\n",
      "   Time: 1.8942625522613525\n",
      "Step 80\n",
      "   Sum: 24510463.4\n",
      "   Loss: 23920728.0\n",
      "   Time: 2.344634771347046\n",
      "Step 100\n",
      "   Sum: 24491962.3\n",
      "   Loss: 18685960.0\n",
      "   Time: 2.6799709796905518\n",
      "Step 120\n",
      "   Sum: 24472509.5\n",
      "   Loss: 15229629.0\n",
      "   Time: 2.7943837642669678\n",
      "Step 140\n",
      "   Sum: 24451727.8\n",
      "   Loss: 12776254.0\n",
      "   Time: 2.7740540504455566\n",
      "Step 160\n",
      "   Sum: 24430970.3\n",
      "   Loss: 10968069.0\n",
      "   Time: 2.87156343460083\n",
      "Step 180\n",
      "   Sum: 24409853.8\n",
      "   Loss: 9567043.0\n",
      "   Time: 3.0004615783691406\n",
      "Step 200\n",
      "   Sum: 24388503.8\n",
      "   Loss: 8461367.0\n",
      "   Time: 2.776614189147949\n",
      "Step 220\n",
      "   Sum: 24366859.3\n",
      "   Loss: 7559129.0\n",
      "   Time: 2.671576499938965\n",
      "Step 240\n",
      "   Sum: 24345091.7\n",
      "   Loss: 6809131.5\n",
      "   Time: 5.651923179626465\n",
      "Step 260\n",
      "   Sum: 24323310.1\n",
      "   Loss: 6173677.0\n",
      "   Time: 5.076333999633789\n",
      "Step 280\n",
      "   Sum: 24301544.2\n",
      "   Loss: 5632199.5\n",
      "   Time: 3.7652931213378906\n",
      "Step 300\n",
      "   Sum: 24279950.0\n",
      "   Loss: 5167559.0\n",
      "   Time: 4.192636013031006\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    with tf.variable_scope('input') as scope:\n",
    "        # use variable instead of placeholder because we're training the intial image to make it\n",
    "        # look like both the content image and the style image\n",
    "        input_image = tf.Variable(np.zeros([1, IMAGE_HEIGHT, IMAGE_WIDTH, 3]), dtype=tf.float32)\n",
    "    \n",
    "    utils.download(VGG_DOWNLOAD_LINK, VGG_MODEL, EXPECTED_BYTES)\n",
    "    utils.make_dir('checkpoints')\n",
    "    utils.make_dir('outputs')\n",
    "    model = vgg_model.load_vgg(VGG_MODEL, input_image)\n",
    "    model['global_step'] = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "    content_image = utils.get_resized_image(CONTENT_IMAGE, IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "    content_image = content_image - MEAN_PIXELS\n",
    "    style_image = utils.get_resized_image(STYLE_IMAGE, IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "    style_image = style_image - MEAN_PIXELS\n",
    "\n",
    "    model['content_loss'], model['style_loss'], model['total_loss'] = _create_losses(model, \n",
    "                                                    input_image, content_image, style_image)\n",
    "    ###############################\n",
    "    ## TO DO: create optimizer\n",
    "    model['optimizer'] = tf.train.AdamOptimizer(LR).minimize(model['total_loss'], \n",
    "                                                            global_step=model['global_step'])\n",
    "    ###############################\n",
    "    model['summary_op'] = _create_summary(model)\n",
    "\n",
    "    initial_image = utils.generate_noise_image(content_image, IMAGE_HEIGHT, IMAGE_WIDTH, NOISE_RATIO)\n",
    "    train(model, input_image, initial_image)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
